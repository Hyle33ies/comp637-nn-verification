# ->->->->-> Primary <-<-<-<-<-
arch: "wrn_28_4" # Use the standard WideResNet 28-4
exp_name: "gtsrb_iter_prune_lwm" # Experiment name for iterative pruning
result_dir: "./trained_models" # Base directory for saving results
num_classes: 43 # GTSRB

# ->->->->-> Pruning (Not used by LWM script directly, but kept for reference) <-<-<-<-<-
# k: N/A # Sparsity is handled iteratively by the main script

# ->->->->-> Train/Finetune (Parameters primarily for ATAS finetuning) <-<-<-<-<-
# trainer: "adv" # Set in the ATAS script itself
epochs: 15 # Default epochs for ATAS fine-tuning per iteration
optimizer: "sgd"
lr: 0.001 # Default learning rate for ATAS fine-tuning
lr_schedule: "step" # Use step decay for fine-tuning
wd: 0.0005 # Weight decay (match ATAS)
momentum: 0.9 # Momentum (match ATAS)
# decay_steps defined in ATAS script

# ->->->->-> Eval (Parameters primarily for ATAS evaluation) <-<-<-<-<-
# val_method: "adv" # Set in the ATAS script itself

# ->->->->-> Dataset <-<-<-<-<-
dataset: GTSRB
batch_size: 64 # Match ATAS training
test_batch_size: 128 # Match ATAS training
data_dir: "../Adv-train/data/GTSRB" # Path relative to ITER_Prune/ directory
data_fraction: 1.0
normalize: True # Use normalization (ATAS uses Normalize layer, but data loader should match)

# ->->->->-> Adv (Parameters primarily for ATAS finetuning/evaluation) <-<-<-<-<-
epsilon: 0.031372549 # 8.0 / 255
# ATAS specific parameters (num_steps, step_size, min/max step etc.) are handled by atas_gtsrb.py

# ->->->->-> Misc <-<-<-<-<-
gpu: "0"
seed: 42
print_freq: 100
