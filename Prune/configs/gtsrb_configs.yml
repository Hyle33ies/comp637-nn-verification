# ->->->->-> Primary <-<-<-<-<-
arch: "wrn_28_4_prune" # Use the adapted architecture for WRN-28-4
exp_name: "gtsrb_hydra_prune_wrn28_4_80" # Experiment name
result_dir: "./trained_models"
num_classes: 43 # GTSRB
exp_mode: "prune" # Set experiment mode to prune
layer_type: "subnet" # Use subnet layers for pruning
init_type: "kaiming_normal"


# ->->->->-> Pruning <-<-<-<-<-
k: 0.2 # 80% pruning = keep 20%

# ->->->->-> Train <-<-<-<-<-
trainer: "adv" # Use adversarial trainer context from pretraining
epochs: 20 # Number of epochs for pruning stage (as per README example)
optimizer: "sgd"
lr: 0.01 # Learning rate for pruning stage
lr_schedule: "cosine" # Or "step"
wd: 0.0001
momentum: 0.9
#warmup
warmup_epochs: 0
warmup_lr: 0.1


# ->->->->-> Eval <-<-<-<-<-
val_method: "adv" # Validate with adversarial attacks


# ->->->->-> Dataset <-<-<-<-<-
dataset: GTSRB # Specify GTSRB dataset
batch_size: 64 # Match pretraining batch size or adjust
test_batch_size: 128 # Match pretraining or adjust
data_dir: "../Adv-train/data/GTSRB" # Path relative to Prune/ directory
data_fraction: 1.0
normalize: True # Use normalization consistent with pretraining
# Specify GTSRB mean/std if different from default or handled in data loader
# mean: [0.485, 0.456, 0.406] # Example, ensure consistency
# std: [0.229, 0.224, 0.225] # Example, ensure consistency


# ->->->->-> Semi-supervised training <-<-<-<-<-
# Not used for this task
semisup_data: "tinyimages"
semisup_fraction: 1.0


# ->->->->-> Adv <-<-<-<-<-
# Use settings consistent with pretraining if relevant for pruning loss/eval
epsilon: 0.031372549 # Actual float value for 8.0 / 255
num_steps: 10 # PGD steps for validation
step_size: 0.007843137 # Actual float value for 2.0 / 255
clip_min: 0
clip_max: 1
distance: "l_inf"
beta: 6.0 # From TRADES, might not be relevant if using standard Adv


# ->->->->-> Misc <-<-<-<-<-
gpu: "0"
seed: 42 # Use a seed for reproducibility
print_freq: 100 
